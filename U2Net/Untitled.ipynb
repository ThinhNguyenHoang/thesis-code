{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1900abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== ARGS PARSER ========================\n",
      "Namespace(batch_size=None, bucket_name=None, data_loading_mode=1, lr=0.002, resume=None, save_interval=None, weights_file=None)\n",
      "loading_mode: (passed, received) (1, 1)\n",
      "SCRIPT IS STARTED WITH ARGS\n",
      "['/dmc/thesis/code/thesis-code/U2Net/trainer/train.py', '--data_loading_mode=1', '--learning_rate=0.002']\n",
      "Model: \"u2netmodel\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 320, 320, 3)]     0         \n",
      "                                                                 \n",
      " u2net (U2NET)               (7, None, 320, 320, 1)    44038669  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,038,669\n",
      "Trainable params: 44,009,869\n",
      "Non-trainable params: 28,800\n",
      "_________________________________________________________________\n",
      "PREPARING DATASET WITH mode: 1\n",
      "DOWLOADING ZIPPED DS: /gcs/thesis-data-bucket/datasets/\n",
      "DATSET ALREADY EXIST: data/DUTS-TR\n",
      "RETURNIN (image_dir, mask_dir)=(data/DUTS-TR/DUTS-TR-Image,data/DUTS-TR/DUTS-TR-Mask)\n",
      "[00000] Loss: 6.6121\n",
      "[00010] Loss: 5.3417\n",
      "[00020] Loss: 3.6220\n",
      "[00030] Loss: 3.2511\n",
      "[00040] Loss: 4.0375\n",
      "[00050] Loss: 3.0339\n",
      "[00060] Loss: 2.8636\n",
      "[00070] Loss: 2.2738\n",
      "[00080] Loss: 2.4401\n",
      "[00090] Loss: 3.1195\n",
      "[00100] Loss: 5.8017\n",
      "[00110] Loss: 2.9958\n",
      "[00120] Loss: 2.0230\n",
      "[00130] Loss: 2.3693\n",
      "[00140] Loss: 3.5025\n",
      "[00150] Loss: 4.2249\n",
      "[00160] Loss: 2.3643\n",
      "[00170] Loss: 4.5730\n",
      "[00180] Loss: 3.4256\n",
      "[00190] Loss: 3.2009\n",
      "[00200] Loss: 2.9564\n",
      "[00210] Loss: 2.2922\n",
      "[00220] Loss: 2.6809\n",
      "[00230] Loss: 3.1489\n",
      "[00240] Loss: 2.6221\n",
      "[00250] Loss: 2.6975\n",
      "[00260] Loss: 3.1197\n",
      "[00270] Loss: 2.5404\n",
      "[00280] Loss: 3.0241\n",
      "[00290] Loss: 2.6124\n",
      "[00300] Loss: 1.9206\n",
      "[00310] Loss: 2.8393\n",
      "[00320] Loss: 2.8026\n",
      "[00330] Loss: 2.2285\n",
      "[00340] Loss: 2.4667\n",
      "[00350] Loss: 2.1094\n",
      "[00360] Loss: 2.2871\n",
      "[00370] Loss: 3.5719\n",
      "[00380] Loss: 2.6582\n",
      "[00390] Loss: 3.6261\n",
      "[00400] Loss: 2.3013\n",
      "[00410] Loss: 2.8048\n",
      "[00420] Loss: 3.3009\n",
      "[00430] Loss: 2.8974\n",
      "[00440] Loss: 2.4352\n",
      "[00450] Loss: 3.4641\n",
      "[00460] Loss: 3.1371\n",
      "[00470] Loss: 2.9222\n",
      "[00480] Loss: 4.4481\n",
      "[00490] Loss: 1.9436\n",
      "[00500] Loss: 3.7498\n",
      "[00510] Loss: 2.6102\n",
      "[00520] Loss: 3.4274\n",
      "[00530] Loss: 2.7287\n",
      "[00540] Loss: 2.4528\n",
      "[00550] Loss: 2.3422\n",
      "[00560] Loss: 2.5638\n",
      "[00570] Loss: 3.1342\n",
      "[00580] Loss: 2.5810\n",
      "[00590] Loss: 2.8001\n",
      "[00600] Loss: 1.9189\n",
      "[00610] Loss: 2.2377\n",
      "[00620] Loss: 2.0482\n",
      "[00630] Loss: 2.8425\n",
      "[00640] Loss: 5.7954\n",
      "[00650] Loss: 3.2834\n",
      "[00660] Loss: 2.9118\n",
      "[00670] Loss: 2.6834\n",
      "[00680] Loss: 2.7636\n",
      "[00690] Loss: 3.6229\n",
      "[00700] Loss: 2.4038\n",
      "[00710] Loss: 2.5832\n",
      "[00720] Loss: 2.4134\n",
      "[00730] Loss: 2.9389\n",
      "[00740] Loss: 3.4063\n",
      "[00750] Loss: 2.0454\n",
      "[00760] Loss: 3.9802\n",
      "[00770] Loss: 2.8015\n",
      "[00780] Loss: 3.5512\n",
      "[00790] Loss: 3.1945\n",
      "[00800] Loss: 3.2064\n",
      "[00810] Loss: 2.9457\n",
      "[00820] Loss: 2.2719\n",
      "[00830] Loss: 2.3252\n",
      "[00840] Loss: 2.1683\n",
      "[00850] Loss: 2.4847\n",
      "[00860] Loss: 4.2445\n",
      "[00870] Loss: 3.5528\n",
      "[00880] Loss: 3.7839\n",
      "[00890] Loss: 2.3957\n",
      "[00900] Loss: 3.2297\n",
      "[00910] Loss: 2.1578\n",
      "[00920] Loss: 3.2088\n",
      "[00930] Loss: 3.2166\n",
      "[00940] Loss: 2.0894\n",
      "[00950] Loss: 2.4376\n",
      "[00960] Loss: 2.2003\n",
      "[00970] Loss: 2.5703\n",
      "[00980] Loss: 3.5784\n",
      "[00990] Loss: 2.2509\n",
      "[01000] Loss: 4.0000\n",
      "Saving state of model to /dmc/thesis/code/thesis-code/U2Net/weights/u2net.h5\n",
      "[01010] Loss: 3.9731\n",
      "[01020] Loss: 2.1075\n",
      "[01030] Loss: 2.3455\n",
      "[01040] Loss: 2.3353\n",
      "[01050] Loss: 1.5972\n",
      "[01060] Loss: 1.4484\n",
      "[01070] Loss: 2.0001\n",
      "[01080] Loss: 2.0679\n",
      "[01090] Loss: 2.8317\n",
      "[01100] Loss: 3.3359\n",
      "[01110] Loss: 3.1096\n",
      "[01120] Loss: 2.1632\n",
      "[01130] Loss: 1.9729\n",
      "[01140] Loss: 4.2895\n",
      "[01150] Loss: 2.8248\n",
      "[01160] Loss: 2.5580\n",
      "[01170] Loss: 1.8939\n",
      "[01180] Loss: 2.6526\n",
      "[01190] Loss: 2.4306\n",
      "[01200] Loss: 3.9878\n",
      "[01210] Loss: 1.8697\n",
      "[01220] Loss: 3.5696\n",
      "[01230] Loss: 2.7605\n",
      "[01240] Loss: 2.9359\n",
      "[01250] Loss: 1.3872\n",
      "[01260] Loss: 4.7986\n",
      "[01270] Loss: 2.9180\n",
      "[01280] Loss: 3.4986\n",
      "[01290] Loss: 2.2065\n",
      "[01300] Loss: 3.5074\n",
      "[01310] Loss: 2.8670\n",
      "[01320] Loss: 5.6838\n",
      "[01330] Loss: 2.2412\n",
      "[01340] Loss: 3.0242\n",
      "[01350] Loss: 2.1906\n",
      "[01360] Loss: 1.6149\n",
      "[01370] Loss: 2.1771\n",
      "[01380] Loss: 3.2057\n",
      "[01390] Loss: 3.5001\n",
      "[01400] Loss: 1.7826\n",
      "[01410] Loss: 2.3099\n",
      "[01420] Loss: 2.4061\n",
      "[01430] Loss: 2.5733\n",
      "[01440] Loss: 1.6863\n",
      "[01450] Loss: 2.6826\n",
      "[01460] Loss: 5.1043\n",
      "[01470] Loss: 3.6186\n",
      "[01480] Loss: 4.1586\n",
      "[01490] Loss: 2.0783\n",
      "[01500] Loss: 1.7563\n",
      "[01510] Loss: 1.7994\n",
      "[01520] Loss: 2.1006\n",
      "[01530] Loss: 3.3558\n",
      "[01540] Loss: 2.0149\n",
      "[01550] Loss: 3.2102\n",
      "[01560] Loss: 2.1655\n",
      "[01570] Loss: 2.8638\n",
      "[01580] Loss: 2.0097\n",
      "[01590] Loss: 2.3255\n",
      "[01600] Loss: 2.7999\n",
      "[01610] Loss: 3.8463\n",
      "[01620] Loss: 3.9852\n",
      "[01630] Loss: 1.7781\n",
      "[01640] Loss: 2.9507\n",
      "[01650] Loss: 2.1923\n",
      "[01660] Loss: 2.4039\n",
      "[01670] Loss: 2.7434\n",
      "[01680] Loss: 2.3445\n",
      "[01690] Loss: 1.4197\n",
      "[01700] Loss: 1.9838\n",
      "[01710] Loss: 2.0357\n",
      "[01720] Loss: 2.5857\n",
      "[01730] Loss: 2.4804\n",
      "[01740] Loss: 2.8022\n",
      "[01750] Loss: 1.8690\n",
      "[01760] Loss: 2.7749\n",
      "[01770] Loss: 2.5910\n",
      "[01780] Loss: 2.0345\n",
      "[01790] Loss: 3.6633\n",
      "[01800] Loss: 2.2479\n",
      "[01810] Loss: 2.7703\n",
      "[01820] Loss: 1.5260\n",
      "[01830] Loss: 4.5427\n",
      "[01840] Loss: 4.1422\n",
      "[01850] Loss: 2.6789\n",
      "[01860] Loss: 2.1009\n",
      "[01870] Loss: 2.4425\n",
      "[01880] Loss: 2.1014\n",
      "[01890] Loss: 2.3917\n",
      "[01900] Loss: 3.0748\n",
      "[01910] Loss: 3.0471\n",
      "[01920] Loss: 2.4667\n",
      "[01930] Loss: 2.8156\n",
      "[01940] Loss: 2.9896\n",
      "[01950] Loss: 1.8350\n",
      "[01960] Loss: 1.7852\n",
      "[01970] Loss: 3.9863\n",
      "[01980] Loss: 2.0757\n",
      "[01990] Loss: 2.2485\n",
      "[02000] Loss: 4.1365\n",
      "Saving state of model to /dmc/thesis/code/thesis-code/U2Net/weights/u2net.h5\n",
      "[02010] Loss: 3.0291\n",
      "[02020] Loss: 2.7527\n",
      "[02030] Loss: 2.7640\n",
      "[02040] Loss: 2.5495\n",
      "[02050] Loss: 4.7333\n",
      "[02060] Loss: 1.5215\n",
      "[02070] Loss: 2.9462\n",
      "[02080] Loss: 3.8108\n",
      "[02090] Loss: 1.6346\n",
      "[02100] Loss: 1.8447\n",
      "[02110] Loss: 2.1428\n",
      "[02120] Loss: 2.3062\n",
      "[02130] Loss: 2.6415\n",
      "[02140] Loss: 2.0773\n",
      "[02150] Loss: 2.2818\n",
      "[02160] Loss: 3.9599\n",
      "[02170] Loss: 2.3203\n",
      "[02180] Loss: 2.3664\n",
      "[02190] Loss: 3.8350\n",
      "[02200] Loss: 2.1267\n",
      "[02210] Loss: 2.1829\n",
      "[02220] Loss: 2.2756\n",
      "[02230] Loss: 1.7322\n",
      "[02240] Loss: 4.1929\n",
      "[02250] Loss: 3.5128\n",
      "[02260] Loss: 1.8741\n",
      "[02270] Loss: 1.6827\n",
      "[02280] Loss: 1.7608\n",
      "[02290] Loss: 3.0942\n",
      "[02300] Loss: 2.1655\n",
      "[02310] Loss: 2.2965\n",
      "[02320] Loss: 2.4957\n",
      "[02330] Loss: 3.6845\n",
      "[02340] Loss: 1.6699\n",
      "[02350] Loss: 5.0000\n",
      "[02360] Loss: 4.5060\n",
      "[02370] Loss: 3.0890\n",
      "[02380] Loss: 1.8754\n",
      "[02390] Loss: 2.5161\n",
      "[02400] Loss: 3.8007\n",
      "[02410] Loss: 5.2384\n",
      "[02420] Loss: 2.0973\n",
      "[02430] Loss: 3.5413\n",
      "[02440] Loss: 2.4892\n",
      "[02450] Loss: 2.2330\n",
      "[02460] Loss: 1.5670\n",
      "[02470] Loss: 4.0496\n",
      "[02480] Loss: 1.9964\n",
      "[02490] Loss: 1.4148\n",
      "[02500] Loss: 1.5315\n",
      "[02510] Loss: 2.0240\n",
      "[02520] Loss: 3.0952\n",
      "[02530] Loss: 4.9806\n",
      "[02540] Loss: 2.2982\n",
      "[02550] Loss: 2.4531\n",
      "[02560] Loss: 2.4907\n",
      "[02570] Loss: 2.0584\n",
      "[02580] Loss: 2.1849\n",
      "[02590] Loss: 1.2550\n",
      "[02600] Loss: 1.9453\n",
      "[02610] Loss: 2.5608\n",
      "[02620] Loss: 2.4845\n",
      "[02630] Loss: 2.3214\n",
      "[02640] Loss: 1.7083\n",
      "[02650] Loss: 1.5634\n",
      "[02660] Loss: 1.8749\n",
      "[02670] Loss: 3.2871\n",
      "[02680] Loss: 4.6007\n",
      "[02690] Loss: 2.2695\n",
      "[02700] Loss: 1.8405\n",
      "[02710] Loss: 1.2468\n",
      "[02720] Loss: 3.4824\n",
      "[02730] Loss: 2.1328\n",
      "[02740] Loss: 1.8791\n",
      "[02750] Loss: 1.5856\n",
      "[02760] Loss: 3.5801\n",
      "[02770] Loss: 2.3081\n",
      "[02780] Loss: 2.5971\n",
      "[02790] Loss: 3.4987\n",
      "[02800] Loss: 2.2032\n",
      "[02810] Loss: 3.3373\n",
      "[02820] Loss: 2.1373\n",
      "[02830] Loss: 3.0062\n",
      "[02840] Loss: 7.1891\n",
      "[02850] Loss: 1.9527\n",
      "[02860] Loss: 2.2272\n",
      "[02870] Loss: 4.3180\n",
      "[02880] Loss: 3.1571\n",
      "[02890] Loss: 1.7700\n",
      "[02900] Loss: 2.3409\n",
      "[02910] Loss: 2.0748\n",
      "[02920] Loss: 1.8433\n",
      "[02930] Loss: 1.6476\n",
      "[02940] Loss: 1.8499\n",
      "[02950] Loss: 2.8568\n",
      "[02960] Loss: 2.5496\n",
      "[02970] Loss: 1.8747\n",
      "[02980] Loss: 2.0461\n",
      "[02990] Loss: 2.4520\n",
      "[03000] Loss: 5.0198\n",
      "Saving state of model to /dmc/thesis/code/thesis-code/U2Net/weights/u2net.h5\n",
      "[03010] Loss: 1.7682\n",
      "[03020] Loss: 2.1796\n",
      "[03030] Loss: 3.6207\n",
      "[03040] Loss: 1.0317\n",
      "[03050] Loss: 2.4349\n",
      "[03060] Loss: 1.4499\n",
      "[03070] Loss: 2.4217\n",
      "[03080] Loss: 1.2918\n",
      "[03090] Loss: 1.6497\n",
      "[03100] Loss: 2.6138\n",
      "[03110] Loss: 1.8830\n",
      "[03120] Loss: 2.0110\n",
      "[03130] Loss: 2.0599\n",
      "[03140] Loss: 2.7467\n",
      "[03150] Loss: 2.3725\n",
      "[03160] Loss: 2.7394\n",
      "[03170] Loss: 2.6890\n",
      "[03180] Loss: 1.2953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03190] Loss: 2.9837\n",
      "[03200] Loss: 2.1457\n",
      "[03210] Loss: 4.0630\n",
      "[03220] Loss: 1.6486\n",
      "[03230] Loss: 1.4909\n",
      "[03240] Loss: 1.2448\n",
      "[03260] Loss: 2.1007\n",
      "[03270] Loss: 2.1431\n",
      "[03280] Loss: 1.5796\n",
      "[03290] Loss: 2.0707\n",
      "[03300] Loss: 1.6996\n",
      "[03310] Loss: 2.2170\n",
      "[03320] Loss: 4.4093\n",
      "[03330] Loss: 0.6767\n",
      "[03340] Loss: 1.9961\n",
      "[03350] Loss: 1.7431\n",
      "[03360] Loss: 1.8804\n",
      "[03370] Loss: 2.0332\n",
      "[03380] Loss: 2.4276\n",
      "[03390] Loss: 1.8162\n",
      "[03400] Loss: 1.9925\n",
      "[03410] Loss: 2.0380\n",
      "[03420] Loss: 2.4817\n",
      "[03430] Loss: 2.4329\n",
      "[03440] Loss: 2.1083\n",
      "[03450] Loss: 1.6781\n",
      "[03460] Loss: 2.6297\n",
      "[03470] Loss: 1.4026\n",
      "[03480] Loss: 2.6841\n",
      "[03490] Loss: 1.7896\n",
      "[03500] Loss: 2.3005\n",
      "[03510] Loss: 1.5598\n",
      "[03520] Loss: 1.2278\n",
      "[03530] Loss: 2.4576\n",
      "[03540] Loss: 1.9371\n",
      "[03550] Loss: 2.5530\n",
      "[03560] Loss: 2.6905\n",
      "[03570] Loss: 2.1558\n",
      "[03580] Loss: 1.4922\n",
      "[03590] Loss: 6.8939\n",
      "[03600] Loss: 1.6468\n",
      "[03610] Loss: 2.9250\n",
      "[03620] Loss: 2.8677\n",
      "[03630] Loss: 4.5962\n",
      "[03640] Loss: 1.9611\n",
      "[03650] Loss: 3.2668\n",
      "[03660] Loss: 2.5755\n",
      "[03670] Loss: 1.2427\n",
      "[03680] Loss: 1.9861\n",
      "[03690] Loss: 1.0800\n",
      "[03700] Loss: 1.1395\n",
      "[03710] Loss: 1.4037\n",
      "[03720] Loss: 1.0306\n",
      "[03730] Loss: 1.0666\n",
      "[03740] Loss: 1.2594\n",
      "[03750] Loss: 2.4820\n",
      "[03760] Loss: 1.9674\n",
      "[03770] Loss: 1.5602\n",
      "[03780] Loss: 2.6119\n",
      "[03790] Loss: 1.6019\n",
      "[03800] Loss: 1.5910\n",
      "[03810] Loss: 1.2406\n",
      "[03820] Loss: 2.4556\n",
      "[03830] Loss: 1.7021\n",
      "[03840] Loss: 2.5280\n",
      "[03850] Loss: 2.1301\n",
      "[03860] Loss: 1.7083\n",
      "[03870] Loss: 3.5765\n",
      "[03880] Loss: 3.6991\n",
      "[03890] Loss: 2.5336\n",
      "[03900] Loss: 1.7770\n",
      "[03910] Loss: 2.5009\n",
      "[03920] Loss: 1.4739\n",
      "[03930] Loss: 2.6705\n",
      "[03940] Loss: 2.6953\n",
      "[03950] Loss: 2.5429\n",
      "[03960] Loss: 1.4488\n",
      "[03970] Loss: 1.7209\n",
      "[03980] Loss: 1.9448\n",
      "[03990] Loss: 1.9351\n",
      "[04000] Loss: 3.8212\n",
      "Saving state of model to /dmc/thesis/code/thesis-code/U2Net/weights/u2net.h5\n",
      "[04010] Loss: 2.1048\n",
      "[04020] Loss: 1.0376\n",
      "[04030] Loss: 2.1563\n",
      "[04040] Loss: 1.0465\n",
      "[04050] Loss: 1.9574\n",
      "[04060] Loss: 1.9584\n",
      "[04070] Loss: 1.1250\n",
      "[04080] Loss: 2.4793\n",
      "[04090] Loss: 0.8780\n",
      "[04100] Loss: 1.8163\n",
      "[04110] Loss: 1.0747\n",
      "[04120] Loss: 1.5044\n",
      "[04130] Loss: 2.2788\n",
      "[04140] Loss: 2.7186\n",
      "[04150] Loss: 1.8486\n",
      "[04160] Loss: 2.1816\n",
      "[04170] Loss: 2.7244\n",
      "[04180] Loss: 1.6364\n",
      "[04190] Loss: 2.9698\n",
      "[04200] Loss: 1.6804\n",
      "[04210] Loss: 2.3845\n",
      "[04220] Loss: 1.8860\n",
      "[04230] Loss: 1.3864\n",
      "[04240] Loss: 1.2166\n",
      "[04250] Loss: 1.2503\n",
      "[04260] Loss: 1.2753\n",
      "[04270] Loss: 1.9063\n",
      "[04280] Loss: 5.6666\n",
      "[04290] Loss: 2.2862\n",
      "[04300] Loss: 2.1434\n",
      "[04310] Loss: 2.7101\n",
      "[04320] Loss: 1.2697\n",
      "[04330] Loss: 1.9970\n",
      "[04340] Loss: 2.2417\n",
      "[04350] Loss: 2.1666\n",
      "[04360] Loss: 1.7784\n",
      "[04370] Loss: 1.0855\n",
      "[04380] Loss: 1.7986\n",
      "[04390] Loss: 1.5107\n",
      "[04400] Loss: 0.7644\n",
      "[04410] Loss: 4.8872\n",
      "[04420] Loss: 1.8339\n",
      "[04430] Loss: 2.7611\n",
      "[04440] Loss: 2.9659\n",
      "[04450] Loss: 1.5372\n",
      "[04460] Loss: 1.7024\n",
      "[04470] Loss: 2.5159\n",
      "[04480] Loss: 1.2564\n",
      "[04490] Loss: 1.5095\n",
      "[04500] Loss: 3.9959\n",
      "[04510] Loss: 5.4850\n",
      "[04520] Loss: 1.5874\n",
      "[04530] Loss: 1.5000\n",
      "[04540] Loss: 0.7596\n",
      "[04550] Loss: 2.1082\n",
      "[04560] Loss: 2.3145\n",
      "[04570] Loss: 2.9499\n",
      "[04580] Loss: 1.4834\n",
      "[04590] Loss: 3.6540\n",
      "[04600] Loss: 3.3408\n",
      "[04610] Loss: 2.1883\n",
      "[04620] Loss: 2.2554\n",
      "[04630] Loss: 2.4041\n",
      "[04640] Loss: 1.0042\n",
      "[04650] Loss: 0.5667\n",
      "[04660] Loss: 1.4832\n",
      "[04670] Loss: 1.3369\n",
      "[04680] Loss: 0.9499\n",
      "[04690] Loss: 3.2429\n",
      "[04700] Loss: 0.7092\n",
      "[04710] Loss: 1.9574\n",
      "[04720] Loss: 1.4405\n",
      "[04730] Loss: 1.6308\n",
      "[04740] Loss: 1.3776\n",
      "[04750] Loss: 1.1316\n",
      "[04760] Loss: 1.0410\n",
      "[04770] Loss: 2.1722\n",
      "[04780] Loss: 1.8899\n",
      "[04790] Loss: 1.0680\n",
      "[04800] Loss: 1.3445\n",
      "[04810] Loss: 1.2180\n",
      "[04820] Loss: 1.6683\n",
      "[04830] Loss: 1.8454\n",
      "[04840] Loss: 2.2783\n",
      "[04850] Loss: 2.6355\n",
      "[04860] Loss: 0.9556\n",
      "[04870] Loss: 1.6992\n",
      "[04880] Loss: 1.8391\n",
      "[04890] Loss: 1.1462\n",
      "[04900] Loss: 2.9056\n",
      "[04910] Loss: 2.6315\n",
      "[04920] Loss: 1.1294\n",
      "[04930] Loss: 1.8979\n",
      "[04940] Loss: 1.7266\n",
      "[04950] Loss: 1.4403\n",
      "[04960] Loss: 1.5241\n",
      "[04970] Loss: 1.1040\n",
      "[04980] Loss: 2.5870\n",
      "[04990] Loss: 2.4703\n",
      "[05000] Loss: 2.7404\n",
      "Saving state of model to /dmc/thesis/code/thesis-code/U2Net/weights/u2net.h5\n",
      "[05010] Loss: 1.5002\n",
      "[05020] Loss: 2.5225\n",
      "[05030] Loss: 0.8488\n",
      "[05040] Loss: 0.8926\n",
      "[05050] Loss: 1.9217\n",
      "[05060] Loss: 1.4068\n",
      "[05070] Loss: 1.5485\n",
      "[05080] Loss: 1.4721\n",
      "[05090] Loss: 2.2366\n",
      "[05100] Loss: 2.2530\n",
      "[05110] Loss: 1.3603\n",
      "[05120] Loss: 2.3751\n",
      "[05130] Loss: 1.7023\n",
      "[05140] Loss: 2.9291\n",
      "[05150] Loss: 1.0849\n",
      "[05160] Loss: 0.8476\n",
      "[05170] Loss: 2.1524\n",
      "[05180] Loss: 0.8984\n",
      "[05190] Loss: 2.0777\n",
      "[05200] Loss: 1.4124\n",
      "[05210] Loss: 1.9972\n",
      "[05220] Loss: 1.7179\n",
      "[05230] Loss: 1.0778\n",
      "[05240] Loss: 2.3953\n",
      "[05250] Loss: 2.0548\n",
      "[05260] Loss: 2.9590\n",
      "[05270] Loss: 2.0966\n",
      "[05280] Loss: 2.6631\n",
      "[05290] Loss: 1.2292\n",
      "[05300] Loss: 1.8527\n",
      "[05310] Loss: 1.8324\n",
      "[05320] Loss: 1.0733\n",
      "[05330] Loss: 2.4304\n",
      "[05340] Loss: 2.2037\n",
      "[05350] Loss: 1.7026\n",
      "[05360] Loss: 1.6713\n",
      "[05370] Loss: 1.4351\n",
      "[05380] Loss: 1.1201\n",
      "[05390] Loss: 3.7537\n",
      "[05400] Loss: 1.6165\n",
      "[05410] Loss: 0.6429\n",
      "[05420] Loss: 0.9198\n",
      "[05430] Loss: 2.1997\n",
      "[05440] Loss: 2.0287\n",
      "[05450] Loss: 7.9437\n",
      "[05460] Loss: 1.7543\n",
      "[05470] Loss: 2.2657\n",
      "[05480] Loss: 1.5609\n",
      "[05490] Loss: 0.9300\n",
      "[05500] Loss: 1.0637\n",
      "[05510] Loss: 0.6958\n",
      "[05520] Loss: 1.1459\n",
      "[05530] Loss: 1.5180\n",
      "[05540] Loss: 1.7152\n",
      "[05550] Loss: 0.9619\n",
      "[05560] Loss: 1.4117\n",
      "[05570] Loss: 1.5319\n",
      "[05580] Loss: 3.2982\n",
      "[05590] Loss: 0.9887\n",
      "[05600] Loss: 2.1537\n",
      "[05610] Loss: 1.7159\n",
      "[05620] Loss: 2.1008\n",
      "[05630] Loss: 2.1960\n",
      "[05640] Loss: 1.2695\n",
      "[05650] Loss: 1.7456\n",
      "[05660] Loss: 1.3429\n",
      "[05670] Loss: 1.9706\n",
      "[05680] Loss: 1.2220\n",
      "[05690] Loss: 1.3879\n",
      "[05700] Loss: 1.5710\n",
      "[05710] Loss: 1.6631\n",
      "[05720] Loss: 1.6447\n",
      "[05730] Loss: 1.2903\n",
      "[05740] Loss: 0.9849\n",
      "[05750] Loss: 2.0240\n",
      "[05760] Loss: 0.8755\n",
      "[05770] Loss: 2.1222\n",
      "[05780] Loss: 1.1369\n",
      "[05790] Loss: 1.7846\n",
      "[05800] Loss: 0.8831\n",
      "[05810] Loss: 4.0027\n",
      "[05820] Loss: 1.0076\n",
      "[05830] Loss: 3.9540\n",
      "[05840] Loss: 1.8688\n",
      "[05850] Loss: 2.7710\n",
      "[05860] Loss: 2.6164\n",
      "[05870] Loss: 1.0008\n",
      "[05880] Loss: 1.0744\n",
      "[05890] Loss: 2.0869\n",
      "[05900] Loss: 6.6146\n",
      "[05920] Loss: 1.9701\n",
      "[05930] Loss: 1.8648\n",
      "[05940] Loss: 1.3880\n",
      "[05950] Loss: 2.7850\n",
      "[05960] Loss: 3.3947\n",
      "[05970] Loss: 1.0864\n",
      "[05980] Loss: 1.2986\n",
      "[05990] Loss: 1.1806\n",
      "[06000] Loss: 1.1467\n",
      "Saving state of model to /dmc/thesis/code/thesis-code/U2Net/weights/u2net.h5\n",
      "[06010] Loss: 1.0918\n",
      "[06020] Loss: 0.9251\n",
      "[06030] Loss: 1.4248\n",
      "[06040] Loss: 0.7549\n",
      "[06050] Loss: 1.7007\n",
      "[06060] Loss: 3.9737\n",
      "[06070] Loss: 2.3092\n",
      "[06080] Loss: 1.9370\n",
      "[06090] Loss: 0.7415\n",
      "[06100] Loss: 1.1784\n",
      "[06110] Loss: 0.8512\n",
      "[06120] Loss: 1.1427\n",
      "[06130] Loss: 0.6855\n",
      "[06140] Loss: 0.9963\n",
      "[06150] Loss: 0.9980\n",
      "[06160] Loss: 3.6870\n",
      "[06170] Loss: 2.6013\n",
      "[06180] Loss: 1.6257\n",
      "[06190] Loss: 1.2187\n",
      "[06200] Loss: 1.3084\n",
      "[06210] Loss: 0.7878\n",
      "[06220] Loss: 0.9743\n",
      "[06230] Loss: 1.7984\n",
      "[06240] Loss: 1.8428\n",
      "[06250] Loss: 0.8470\n",
      "[06260] Loss: 1.1186\n",
      "[06270] Loss: 3.0030\n",
      "[06280] Loss: 1.6601\n",
      "[06290] Loss: 1.0225\n",
      "[06300] Loss: 1.9722\n",
      "[06310] Loss: 0.9312\n",
      "[06320] Loss: 2.0874\n",
      "[06330] Loss: 1.7840\n",
      "[06340] Loss: 1.6158\n",
      "[06350] Loss: 1.7593\n",
      "[06360] Loss: 1.7205\n",
      "[06370] Loss: 1.3174\n",
      "[06380] Loss: 1.2270\n",
      "[06390] Loss: 1.4141\n",
      "[06400] Loss: 0.9296\n",
      "[06410] Loss: 1.3650\n",
      "[06420] Loss: 0.9108\n",
      "[06430] Loss: 0.7907\n",
      "[06440] Loss: 1.6758\n",
      "[06450] Loss: 1.8948\n",
      "[06460] Loss: 1.4994\n",
      "[06470] Loss: 1.2904\n",
      "[06480] Loss: 1.0805\n",
      "[06490] Loss: 1.5782\n",
      "[06500] Loss: 0.5717\n",
      "[06510] Loss: 1.8186\n",
      "[06520] Loss: 2.0071\n",
      "[06530] Loss: 1.4896\n",
      "[06540] Loss: 3.5892\n",
      "[06550] Loss: 1.7376\n",
      "[06560] Loss: 2.4590\n",
      "[06570] Loss: 3.3696\n",
      "[06580] Loss: 1.1507\n",
      "[06590] Loss: 0.8610\n",
      "[06600] Loss: 1.4636\n",
      "[06610] Loss: 3.3625\n",
      "[06620] Loss: 2.0531\n",
      "[06630] Loss: 2.2921\n",
      "[06640] Loss: 1.6404\n",
      "[06650] Loss: 2.0461\n",
      "[06660] Loss: 0.8608\n",
      "[06670] Loss: 1.3573\n",
      "[06680] Loss: 0.9226\n",
      "[06690] Loss: 0.9799\n",
      "[06700] Loss: 0.9903\n",
      "[06710] Loss: 1.3087\n",
      "[06720] Loss: 1.5297\n",
      "[06730] Loss: 1.3372\n",
      "[06740] Loss: 1.7753\n",
      "[06750] Loss: 1.2848\n",
      "[06760] Loss: 2.3118\n",
      "[06770] Loss: 0.8216\n",
      "[06780] Loss: 3.6410\n",
      "[06790] Loss: 1.3529\n",
      "[06800] Loss: 1.1391\n",
      "[06810] Loss: 2.3837\n",
      "[06820] Loss: 2.3283\n",
      "[06830] Loss: 1.2528\n",
      "[06840] Loss: 3.5304\n",
      "[06850] Loss: 2.3389\n",
      "[06860] Loss: 1.0493\n",
      "[06870] Loss: 1.6566\n",
      "[06880] Loss: 2.9048\n",
      "[06890] Loss: 0.9686\n",
      "[06900] Loss: 0.7038\n",
      "[06910] Loss: 2.0598\n",
      "[06920] Loss: 1.1806\n",
      "[06930] Loss: 2.1975\n",
      "[06940] Loss: 2.8135\n",
      "[06950] Loss: 0.8155\n",
      "[06960] Loss: 0.9696\n",
      "[06970] Loss: 1.8495\n",
      "[06980] Loss: 0.9577\n",
      "[06990] Loss: 1.2663\n",
      "[07000] Loss: 0.9648\n",
      "Saving state of model to /dmc/thesis/code/thesis-code/U2Net/weights/u2net.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07010] Loss: 1.2187\n",
      "[07020] Loss: 0.8256\n",
      "[07030] Loss: 1.8500\n",
      "[07040] Loss: 1.3583\n",
      "[07050] Loss: 1.2347\n",
      "[07060] Loss: 0.4260\n",
      "[07070] Loss: 1.9642\n",
      "[07080] Loss: 1.0736\n",
      "[07090] Loss: 1.1270\n",
      "[07100] Loss: 1.4062\n",
      "[07110] Loss: 0.9102\n",
      "[07120] Loss: 0.9210\n",
      "[07130] Loss: 0.9451\n",
      "[07140] Loss: 1.6087\n",
      "[07150] Loss: 0.8626\n",
      "[07160] Loss: 1.4125\n",
      "[07170] Loss: 1.4141\n",
      "[07180] Loss: 1.1692\n",
      "[07190] Loss: 1.4994\n",
      "[07200] Loss: 1.2807\n",
      "[07210] Loss: 2.0318\n",
      "[07220] Loss: 1.4034\n",
      "[07230] Loss: 3.0945\n",
      "[07240] Loss: 3.4638\n",
      "[07250] Loss: 1.2285\n",
      "[07260] Loss: 0.9215\n",
      "[07270] Loss: 2.0622\n",
      "[07280] Loss: 1.6345\n",
      "[07290] Loss: 0.7009\n",
      "[07300] Loss: 0.7750\n",
      "[07310] Loss: 2.1920\n",
      "[07320] Loss: 1.1514\n",
      "[07330] Loss: 3.6682\n",
      "[07340] Loss: 4.4248\n",
      "[07350] Loss: 3.1993\n",
      "[07360] Loss: 1.1104\n",
      "[07370] Loss: 3.8852\n",
      "[07380] Loss: 1.4186\n",
      "[07390] Loss: 1.8119\n",
      "[07400] Loss: 0.9718\n",
      "[07410] Loss: 1.4562\n",
      "[07420] Loss: 2.4517\n",
      "[07430] Loss: 2.0315\n",
      "[07440] Loss: 1.1272\n",
      "[07450] Loss: 5.0313\n",
      "[07460] Loss: 2.1718\n",
      "[07470] Loss: 2.1470\n",
      "[07480] Loss: 2.4358\n",
      "[07490] Loss: 1.4649\n",
      "[07500] Loss: 0.7378\n",
      "[07510] Loss: 1.6594\n",
      "[07520] Loss: 3.8121\n",
      "[07530] Loss: 0.9580\n",
      "[07540] Loss: 1.4291\n",
      "[07550] Loss: 0.7744\n",
      "[07560] Loss: 1.7513\n",
      "[07570] Loss: 1.7530\n",
      "[07580] Loss: 1.1174\n",
      "[07590] Loss: 1.3623\n",
      "[07600] Loss: 2.7818\n",
      "[07610] Loss: 2.0569\n",
      "[07620] Loss: 1.6536\n",
      "[07630] Loss: 2.4731\n",
      "[07640] Loss: 1.6093\n",
      "[07650] Loss: 2.2693\n",
      "[07660] Loss: 0.8112\n",
      "[07670] Loss: 0.7827\n",
      "[07680] Loss: 1.0109\n",
      "[07690] Loss: 1.0692\n",
      "[07700] Loss: 0.5830\n",
      "[07710] Loss: 1.3845\n",
      "[07720] Loss: 1.8778\n",
      "[07730] Loss: 3.9537\n",
      "[07740] Loss: 1.1947\n",
      "[07750] Loss: 0.6804\n",
      "[07760] Loss: 0.8844\n",
      "[07770] Loss: 2.1827\n",
      "[07780] Loss: 4.6708\n",
      "[07790] Loss: 1.1334\n",
      "[07800] Loss: 1.0587\n",
      "[07810] Loss: 1.2066\n",
      "[07820] Loss: 2.8303\n",
      "[07830] Loss: 3.1773\n",
      "[07840] Loss: 3.2150\n",
      "[07850] Loss: 1.1170\n",
      "[07860] Loss: 1.1884\n",
      "[07870] Loss: 0.7783\n",
      "[07880] Loss: 0.9761\n",
      "[07890] Loss: 0.9229\n",
      "[07900] Loss: 1.9975\n",
      "[07910] Loss: 1.6151\n",
      "[07920] Loss: 1.5319\n",
      "[07930] Loss: 1.1499\n",
      "[07940] Loss: 2.1195\n",
      "[07950] Loss: 0.8772\n",
      "[07960] Loss: 2.3879\n",
      "[07970] Loss: 3.2462\n",
      "[07980] Loss: 0.7479\n",
      "[07990] Loss: 0.9893\n",
      "[08000] Loss: 3.0492\n",
      "Saving state of model to /dmc/thesis/code/thesis-code/U2Net/weights/u2net.h5\n",
      "[08010] Loss: 1.4684\n",
      "[08020] Loss: 0.7005\n",
      "[08030] Loss: 1.0592\n",
      "[08040] Loss: 0.7253\n",
      "[08050] Loss: 1.2329\n",
      "[08060] Loss: 3.0273\n",
      "[08070] Loss: 1.5281\n",
      "[08080] Loss: 1.4343\n",
      "[08090] Loss: 1.1396\n",
      "[08100] Loss: 1.9450\n",
      "[08110] Loss: 1.8355\n",
      "[08120] Loss: 1.5540\n",
      "[08130] Loss: 1.4299\n",
      "[08140] Loss: 1.4841\n",
      "[08150] Loss: 0.8537\n",
      "[08160] Loss: 1.7700\n",
      "[08170] Loss: 0.8974\n",
      "[08180] Loss: 1.3712\n",
      "[08190] Loss: 2.0899\n",
      "[08200] Loss: 1.0884\n",
      "[08210] Loss: 0.6166\n",
      "[08220] Loss: 1.6283\n",
      "[08230] Loss: 2.7414\n",
      "[08240] Loss: 1.4922\n",
      "[08250] Loss: 1.2481\n",
      "[08260] Loss: 1.6863\n",
      "[08270] Loss: 0.7691\n",
      "[08280] Loss: 1.7474\n",
      "[08290] Loss: 1.1828\n",
      "[08300] Loss: 1.7990\n",
      "[08310] Loss: 2.0207\n",
      "[08320] Loss: 1.3641\n",
      "[08330] Loss: 1.9266\n",
      "[08340] Loss: 0.7501\n",
      "[08350] Loss: 1.8690\n",
      "[08360] Loss: 2.0220\n",
      "[08370] Loss: 1.6341\n",
      "[08380] Loss: 2.4621\n",
      "[08390] Loss: 1.3192\n",
      "[08400] Loss: 1.6381\n",
      "[08410] Loss: 1.7351\n",
      "[08420] Loss: 0.6525\n",
      "[08430] Loss: 1.2663\n",
      "[08440] Loss: 0.6563\n",
      "[08450] Loss: 2.9045\n",
      "[08460] Loss: 0.7678\n",
      "[08470] Loss: 1.9511\n",
      "[08480] Loss: 0.7250\n",
      "[08490] Loss: 1.1650\n",
      "[08500] Loss: 2.1646\n",
      "[08510] Loss: 1.3836\n",
      "[08520] Loss: 1.0045\n",
      "[08530] Loss: 2.8917\n",
      "[08540] Loss: 0.9020\n",
      "[08550] Loss: 0.9123\n",
      "[08560] Loss: 1.7072\n",
      "[08570] Loss: 1.2639\n",
      "[08580] Loss: 2.2732\n",
      "[08590] Loss: 1.2885\n",
      "[08600] Loss: 1.2380\n",
      "[08610] Loss: 2.7084\n",
      "[08620] Loss: 3.1088\n",
      "[08630] Loss: 3.0196\n",
      "[08640] Loss: 2.3714\n",
      "[08650] Loss: 0.6859\n",
      "[08660] Loss: 1.2149\n",
      "[08670] Loss: 1.4993\n",
      "[08680] Loss: 0.5178\n",
      "[08690] Loss: 3.8795\n",
      "[08700] Loss: 0.8314\n",
      "[08710] Loss: 1.5553\n",
      "[08720] Loss: 1.0934\n",
      "[08730] Loss: 1.9535\n",
      "[08740] Loss: 1.0749\n",
      "[08750] Loss: 1.4518\n",
      "[08760] Loss: 1.7043\n",
      "[08770] Loss: 0.5379\n",
      "[08780] Loss: 1.0065\n",
      "[08790] Loss: 0.5129\n",
      "[08800] Loss: 3.0305\n",
      "[08810] Loss: 0.8736\n",
      "[08820] Loss: 1.3247\n",
      "[08830] Loss: 1.1902\n",
      "[08840] Loss: 1.1434\n",
      "[08850] Loss: 1.1764\n",
      "[08860] Loss: 4.4891\n",
      "[08870] Loss: 1.6189\n",
      "[08880] Loss: 0.5689\n",
      "[08890] Loss: 1.6712\n",
      "[08900] Loss: 1.5718\n",
      "[08910] Loss: 1.6202\n",
      "[08920] Loss: 2.4437\n",
      "[08930] Loss: 0.8559\n",
      "[08940] Loss: 1.2753\n",
      "[08950] Loss: 1.1159\n",
      "[08960] Loss: 1.0849\n",
      "[08970] Loss: 5.0216\n",
      "[08980] Loss: 1.0001\n",
      "[08990] Loss: 0.4131\n",
      "[09000] Loss: 2.2374\n",
      "Saving state of model to /dmc/thesis/code/thesis-code/U2Net/weights/u2net.h5\n",
      "[09010] Loss: 1.6534\n",
      "[09020] Loss: 0.5956\n",
      "[09030] Loss: 0.7839\n",
      "[09040] Loss: 2.7544\n",
      "[09050] Loss: 1.2623\n",
      "[09060] Loss: 1.4497\n",
      "[09070] Loss: 0.9328\n",
      "[09080] Loss: 4.1747\n",
      "[09090] Loss: 1.8384\n",
      "[09100] Loss: 1.1460\n",
      "[09110] Loss: 1.1993\n",
      "[09120] Loss: 2.0254\n",
      "[09130] Loss: 1.8161\n",
      "[09140] Loss: 1.1083\n",
      "[09150] Loss: 0.8897\n",
      "[09160] Loss: 2.3046\n",
      "[09170] Loss: 0.8373\n",
      "[09180] Loss: 1.6562\n",
      "[09190] Loss: 1.0202\n",
      "[09200] Loss: 2.4792\n",
      "[09210] Loss: 1.5227\n",
      "[09220] Loss: 1.4048\n",
      "[09230] Loss: 1.1331\n",
      "[09240] Loss: 0.7931\n",
      "[09250] Loss: 1.7922\n",
      "[09260] Loss: 2.0021\n",
      "[09270] Loss: 0.8783\n",
      "[09280] Loss: 1.7047\n",
      "[09290] Loss: 2.0124\n",
      "[09300] Loss: 1.4805\n",
      "[09310] Loss: 0.7113\n",
      "[09320] Loss: 1.5160\n",
      "[09330] Loss: 1.9260\n",
      "[09340] Loss: 1.0869\n",
      "[09350] Loss: 1.3259\n",
      "[09360] Loss: 0.7435\n",
      "[09370] Loss: 1.0690\n",
      "[09380] Loss: 0.5649\n",
      "[09390] Loss: 2.8580\n",
      "[09400] Loss: 0.6527\n",
      "[09410] Loss: 0.9764\n",
      "[09420] Loss: 0.7935\n",
      "[09430] Loss: 1.0687\n",
      "[09440] Loss: 3.1668\n",
      "[09450] Loss: 1.4969\n",
      "[09460] Loss: 2.1596\n",
      "[09470] Loss: 1.0273\n",
      "[09480] Loss: 1.4971\n",
      "[09490] Loss: 0.8794\n",
      "[09500] Loss: 1.6379\n",
      "[09510] Loss: 2.0371\n",
      "[09520] Loss: 0.8906\n",
      "[09530] Loss: 2.1429\n",
      "[09540] Loss: 0.5983\n",
      "[09550] Loss: 1.7952\n",
      "[09560] Loss: 1.3021\n",
      "[09570] Loss: 1.9390\n",
      "[09580] Loss: 1.1290\n",
      "[09590] Loss: 0.8183\n",
      "[09600] Loss: 0.7164\n",
      "[09610] Loss: 1.4857\n",
      "[09620] Loss: 1.2874\n",
      "[09630] Loss: 1.6648\n",
      "[09640] Loss: 0.7170\n",
      "[09650] Loss: 1.1853\n",
      "[09660] Loss: 0.7634\n",
      "[09670] Loss: 1.5108\n",
      "[09680] Loss: 5.1876\n",
      "[09690] Loss: 2.1045\n",
      "[09700] Loss: 4.6725\n",
      "[09710] Loss: 1.5269\n",
      "[09720] Loss: 1.5210\n",
      "[09730] Loss: 0.9420\n",
      "[09740] Loss: 0.7186\n",
      "[09750] Loss: 1.9771\n",
      "[09760] Loss: 1.3386\n",
      "[09770] Loss: 1.0786\n",
      "[09780] Loss: 0.6778\n",
      "[09790] Loss: 1.8593\n",
      "[09800] Loss: 1.2171\n",
      "[09810] Loss: 1.1627\n",
      "[09820] Loss: 3.4463\n",
      "[09830] Loss: 2.4542\n",
      "[09840] Loss: 1.5516\n",
      "[09850] Loss: 1.0742\n",
      "[09860] Loss: 0.8060\n",
      "[09870] Loss: 0.9107\n",
      "[09880] Loss: 1.2312\n",
      "[09890] Loss: 1.2740\n",
      "[09900] Loss: 0.9900\n",
      "[09910] Loss: 1.9844\n",
      "[09920] Loss: 1.4928\n",
      "[09930] Loss: 1.0457\n",
      "[09940] Loss: 0.7273\n",
      "[09950] Loss: 1.6876\n",
      "[09960] Loss: 1.3030\n",
      "[09970] Loss: 1.7277\n",
      "[09980] Loss: 5.8103\n",
      "[09990] Loss: 1.7521\n"
     ]
    }
   ],
   "source": [
    "!python -m trainer.train --data_loading_mode=1 --learning_rate=0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2815e5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9657 sha256=4bbc43279aecab6f7f3fb056eca1ac60af6d8e7ef58707bc2b4ac3a216c2b276\n",
      "  Stored in directory: /root/.cache/pip/wheels/bd/a8/c3/3cf2c14a1837a4e04bd98631724e81f33f462d86a1d895fae0\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
      "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/console.py\", line 1673, in print\n",
      "    extend(render(renderable, render_options))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/console.py\", line 1305, in render\n",
      "    for render_output in iter_render:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/logging.py\", line 134, in __rich_console__\n",
      "    for line in lines:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/segment.py\", line 249, in split_lines\n",
      "    for segment in segments:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/console.py\", line 1283, in render\n",
      "    renderable = rich_cast(renderable)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/protocol.py\", line 36, in rich_cast\n",
      "    renderable = cast_method()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/self_outdated_check.py\", line 130, in __rich__\n",
      "    pip_cmd = get_best_invocation_for_this_pip()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/entrypoints.py\", line 58, in get_best_invocation_for_this_pip\n",
      "    if found_executable and os.path.samefile(\n",
      "  File \"/usr/lib/python3.8/genericpath.py\", line 101, in samefile\n",
      "    s2 = os.stat(f2)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/pip'\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/__main__.py\", line 31, in <module>\n",
      "    sys.exit(_main())\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/main.py\", line 70, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
      "    return self._main(args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
      "    self.handle_pip_version_check(options)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/req_command.py\", line 190, in handle_pip_version_check\n",
      "    pip_self_version_check(session, options)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/self_outdated_check.py\", line 236, in pip_self_version_check\n",
      "    logger.warning(\"[present-rich] %s\", upgrade_prompt)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1458, in warning\n",
      "    self._log(WARNING, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1589, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1599, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1661, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 954, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/logging.py\", line 179, in emit\n",
      "    self.handleError(record)\n",
      "Message: '[present-rich] %s'\n",
      "Arguments: (UpgradePrompt(old='22.2.2', new='23.0.1'),)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ce0cd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dmc/trainer\n"
     ]
    }
   ],
   "source": [
    "%cd trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fac9017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\t\t\t  sun_bpdgwtepgxqovhqz.jpg  sun_bwbhnfcnofofbvln.jpg\r\n",
      "..\t\t\t  sun_bphgjrvusnbakems.jpg  sun_bwqkcopmnmjftesd.jpg\r\n",
      "sun_blevcquybnxhyqpo.jpg  sun_bpkzxgzfcbqufuhl.jpg  sun_bwuqigvlvwdpvgxf.jpg\r\n",
      "sun_blewytlvnsaknhfm.jpg  sun_bpskwxthudnpsjsx.jpg  sun_bxukkxzbyvgrppga.jpg\r\n",
      "sun_bloiepetajcmgjea.jpg  sun_bptvsklugzthdsvr.jpg  sun_bxyuukfdhtbdfalo.jpg\r\n",
      "sun_blrxwxozhbwwbomb.jpg  sun_bqxmpqprvceuukrp.jpg  sun_bycbslbkklxwjzae.jpg\r\n",
      "sun_bltxvefsfjcczkep.jpg  sun_brbymiqqcfkijqko.jpg  sun_byokesjvhivrbksy.jpg\r\n",
      "sun_bmaxfxfzjpcbfbzv.jpg  sun_brtkvwdzmofnumti.jpg  sun_byrhmeymctatbmyh.jpg\r\n",
      "sun_bmhzdbzfwpzapfdm.jpg  sun_bsfoombdzwukkrwr.jpg  sun_bysimhtepnxaoqew.jpg\r\n",
      "sun_bmibzryxgjxdkkmz.jpg  sun_bsgecqaxotjmdmtc.jpg  sun_bzgnktlmvljaawtx.jpg\r\n",
      "sun_bmkeidmsbrgldpyu.jpg  sun_bskdxgivhzoyczkm.jpg  sun_bzphsbodnobynmql.jpg\r\n",
      "sun_bmncqthmkuozzoer.jpg  sun_bsxucgelzldkmasn.jpg  sun_bzpplgfcoucekaud.jpg\r\n",
      "sun_bnccwkfpedfyjptv.jpg  sun_btmxdkrnymjzbcnj.jpg  sun_bzqfyfzrykaaoxra.jpg\r\n",
      "sun_bnmfouesrgmbklgf.jpg  sun_btpaokvdlqedxmdq.jpg  sun_bzqnwbjyvhmotvcu.jpg\r\n",
      "sun_bnqvikfasjffokmx.jpg  sun_btslbdkuhizidarp.jpg  sun_cceeapoklvyrjgte.jpg\r\n",
      "sun_bnskyqooqopydrxr.jpg  sun_btzdowcyqiyizeei.jpg  sun_cctsbovvkqbwpxgn.jpg\r\n",
      "sun_bnurgxuweprtmrwc.jpg  sun_buftmindmopkxolf.jpg  sun_cgldefwbwikfpyrh.jpg\r\n",
      "sun_bohljlqgdtydhjdy.jpg  sun_bujfgxiysguzentt.jpg  sun_cusxlgfeeeicytuv.jpg\r\n",
      "sun_borkhuoyzurajaun.jpg  sun_bulfgxlqkjpegkuu.jpg  sun_dhdzmvwgtsedkkgv.jpg\r\n",
      "sun_boujnixvugpagelu.jpg  sun_butoyfxzjbeuwpzf.jpg  sun_dielyavhzjucptwl.jpg\r\n",
      "sun_bounwtxdrpdpwpur.jpg  sun_bvbkdowsldfeebql.jpg  sun_dnxhxsnxwjesksua.jpg\r\n",
      "sun_bozkabojwlnripdl.jpg  sun_bvluocmpqsvuwgpq.jpg  test_dzkggnowaqnfrorl.jpg\r\n"
     ]
    }
   ],
   "source": [
    "!export DISPLAY=:0\n",
    "\n",
    "\n",
    "!ls -a local_test_data/DUTS-TR/DUTS-TR-Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9a7810b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@0.028] global /usr/local/src/opencv/modules/imgcodecs/src/loadsave.cpp (239) findDecoder imread_('local_test_data/DUTS-TR/DUTS-TR-Image/sun_bpdgwtepgxqovhqz.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      4\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_test_data/DUTS-TR/DUTS-TR-Image/sun_bpdgwtepgxqovhqz.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, img\u001b[38;5;241m.\u001b[39msize)\n\u001b[1;32m      6\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m, img)\n\u001b[1;32m      7\u001b[0m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('local_test_data/DUTS-TR/DUTS-TR-Image/sun_bpdgwtepgxqovhqz.jpg')\n",
    "print(img.shape, \" \", img.size)\n",
    "cv2.imshow('image', img)\n",
    "cv2.waitKey(0) & 0xFF\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cac7958",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeac9b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-fb52792c0fcdbcd7\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-fb52792c0fcdbcd7\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e17bc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\t    data\t\tlogs\t\t     trainer\r\n",
      "..\t    Dockerfile\t\tsetup.py\t     Untitled.ipynb\r\n",
      "config.yml  .ipynb_checkpoints\ttrain_build_test.sh  weights\r\n"
     ]
    }
   ],
   "source": [
    "!ls -a "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
